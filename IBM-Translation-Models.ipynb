{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWyIXsUL8jwZ"
   },
   "source": [
    "# Implementation of Cross Language Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v0fU6VvNWBG"
   },
   "source": [
    "Importing the necessary libraries for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.079790Z",
     "start_time": "2019-11-15T14:03:03.058204Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3270,
     "status": "ok",
     "timestamp": 1573230069920,
     "user": {
      "displayName": "VITTHAL BHANDARI",
      "photoUrl": "",
      "userId": "13632601739833432191"
     },
     "user_tz": -330
    },
    "id": "fZV7-_cQMwPk",
    "outputId": "cffc011d-702e-401a-abbb-4413550542f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n",
      "[nltk_data]     (_ssl.c:852)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7fUCd1_NmKd"
   },
   "source": [
    "Running the whole EM-algorithm for maximun of 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.258368Z",
     "start_time": "2019-11-15T14:03:03.082071Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xRzqLNXBMwPs"
   },
   "outputs": [],
   "source": [
    "max_num_of_iterations = 10 \n",
    "\n",
    "def converge_limit(mat,mat_old,num_of_iterations):\n",
    "    epsilon = 0.0000001\n",
    "    if num_of_iterations > max_num_of_iterations :\n",
    "        return True\n",
    "\n",
    "    for i in range(len(mat)) :\n",
    "        for j in range(len(mat[0])) :\n",
    "            if math.fabs(mat[i][j] - mat_old[i][j]) > epsilon:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKgnUewsN3TX"
   },
   "source": [
    "This is the EM step required in IBM Model 1.\n",
    "EM Algorithm consists of two steps:\n",
    "1. Expectation-Step: Apply model to the data\n",
    "      using the model, assign probabilities to possible values\n",
    "2. Maximization-Step: Estimate model from data\n",
    "      (a) take assign values as fact\n",
    "      (b) collect counts (weighted by probabilities)\n",
    "      (c) estimate model from counts\n",
    "\n",
    "We have also incorporated laplace smoothing as a way to improve accuracy of IBM model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.347016Z",
     "start_time": "2019-11-15T14:03:03.266226Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aO3LLWFuMwPx"
   },
   "outputs": [],
   "source": [
    "def ibm_model_1(dutch_sentences,english_sentences,dutch_word_dict,english_word_dict):\n",
    "    print(\"\\nIBMModel 1 Training\")\n",
    "    num_of_dut_word = len(dutch_word_dict)\n",
    "    num_of_eng_word = len(english_word_dict)\n",
    "    # EM algorithm\n",
    "    t_e_f_mat = np.full((len(dutch_word_dict), len(english_word_dict)), 1 / len(english_word_dict), dtype=float)\n",
    "    t_e_f_mat_temp = np.full((len(dutch_word_dict), len(english_word_dict)), 1, dtype=float)\n",
    "\n",
    "    cnt_iter = 0\n",
    "    while not converge_limit(t_e_f_mat,t_e_f_mat_temp,cnt_iter) :\n",
    "        cnt_iter += 1\n",
    "        print(\"Iteration : \", cnt_iter)\n",
    "        t_e_f_mat_temp = t_e_f_mat.copy()\n",
    "        count_e_f = np.full((len(dutch_word_dict), len(english_word_dict)), 0, dtype=float)\n",
    "        total_f = np.full((len(english_word_dict)),0, dtype=float)\n",
    "\n",
    "        print(str(datetime.now()))\n",
    "        for idx_dut, dut_sen in enumerate(dutch_sentences): \n",
    "            # Compute Normalization\n",
    "            dut_sen_words = dut_sen.split(\" \")\n",
    "            s_total = np.full((len(dut_sen_words)),0,dtype=float)\n",
    "            for idx_word in range(len(dut_sen_words)): \n",
    "                dut_word = dut_sen_words[idx_word]\n",
    "                if dut_word == '':\n",
    "                    continue \n",
    "                s_total[idx_word] = 0\n",
    "                eng_sen_words = english_sentences[idx_dut].split(\" \")\n",
    "                for eng_word in eng_sen_words: \n",
    "                    if eng_word == '' :\n",
    "                        continue \n",
    "                    idx_dut_in_dict =dutch_word_dict[dut_word]\n",
    "                    idx_eng_in_dict = english_word_dict[eng_word]\n",
    "                    s_total[idx_word] += t_e_f_mat[idx_dut_in_dict][idx_eng_in_dict]\n",
    "                \n",
    "\n",
    "            # Collect Counts\n",
    "            dut_sen_words = dut_sen.split(\" \")\n",
    "            for idx_word in range(len(dut_sen_words)):\n",
    "                if dut_word == '':\n",
    "                    continue \n",
    "                dut_word = dut_sen_words[idx_word]\n",
    "                eng_sen_words = english_sentences[idx_dut].split(\" \")\n",
    "                for eng_word in eng_sen_words:\n",
    "                    if eng_word == '' :\n",
    "                        continue\n",
    "                    idx_dut_in_dict =dutch_word_dict[dut_word]\n",
    "                    idx_eng_in_dict = english_word_dict[eng_word]\n",
    "                    count_e_f[idx_dut_in_dict][idx_eng_in_dict] += t_e_f_mat[idx_dut_in_dict][idx_eng_in_dict] / s_total[idx_word]\n",
    "                    total_f[idx_eng_in_dict] += t_e_f_mat[idx_dut_in_dict][idx_eng_in_dict] / s_total[idx_word]\n",
    "                \n",
    "        # Estimate Probabilities without Laplace Smoothing\n",
    "#         for eng_idx in  range(num_of_eng_word): \n",
    "#             for dut_idx in range(num_of_dut_word): \n",
    "#                 if count_e_f[dut_idx][eng_idx] != 0 :\n",
    "#                     t_e_f_mat[dut_idx][eng_idx] = (count_e_f[dut_idx][eng_idx]) / (total_f[eng_idx]) \n",
    "        \n",
    "        # Estimate Probabilities with Laplace Smoothing\n",
    "        for eng_idx in  range(num_of_eng_word): #for all foreign words f do\n",
    "            for dut_idx in range(num_of_dut_word): #for all English words e do\n",
    "                if count_e_f[dut_idx][eng_idx] != 0 :\n",
    "                    t_e_f_mat[dut_idx][eng_idx] = (count_e_f[dut_idx][eng_idx] + 1) / (total_f[eng_idx] + num_of_eng_word) \n",
    "            \n",
    "        print(str(datetime.now()))\n",
    "    \n",
    "    print(\"IBMModel1 Training Complete !\")\n",
    "    return t_e_f_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-n6QsV1zAbd9"
   },
   "source": [
    "The IBM Model 2 has an additional model for alignment that is not present in Model 1.The IBM Model 2 addressed this issue by modeling the translation of a foreign input word in position i to a native language word in position j using an alignment probability distribution defined as:\n",
    "\n",
    "a(i or j, l_e, l_f)\n",
    "\n",
    "We have also incorporated laplace smoothing as a way to improve accuracy of IBM model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.437624Z",
     "start_time": "2019-11-15T14:03:03.354619Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Lm3eHDRR8jwo"
   },
   "outputs": [],
   "source": [
    "def ibm_model_2(t_e_f_mat,dutch_sentences,english_sentences,dutch_word_dict,english_word_dict,max_le,max_lf):\n",
    "    print(\"\\nIBMModel 2 Training \")\n",
    "    a_i_le_lf_mat = np.zeros((max_lf, max_le, max_lf,max_le), dtype=float)\n",
    "\n",
    "    for lf in range(max_lf):\n",
    "        a_i_le_lf_mat[:,:,lf,:] = 1/(lf+1)\n",
    "\n",
    "    num_of_e_word = len(dutch_word_dict)\n",
    "    num_of_f_word = len(english_word_dict)\n",
    "\n",
    "    t_e_f_mat_prev = np.full((num_of_e_word, num_of_f_word), 1,dtype=float)\n",
    "    cnt_iter = 0\n",
    "\n",
    "    while not converge_limit(t_e_f_mat,t_e_f_mat_prev,cnt_iter) :\n",
    "        cnt_iter += 1\n",
    "        print(\"Iteration : \", cnt_iter)\n",
    "        t_e_f_mat_prev = t_e_f_mat.copy()\n",
    "        count_e_f = np.full((num_of_e_word, num_of_f_word), 0, dtype=float)\n",
    "        total_f = np.full((num_of_f_word),0, dtype=float)\n",
    "        count_a_i_le_lf = np.zeros((max_lf, max_le, max_lf,max_le), dtype=float)\n",
    "        total_a_j_le_lf = np.zeros((max_le,max_le,max_lf),dtype=float)\n",
    "\n",
    "        print(str(datetime.now()))\n",
    "        for idx_e, e_sen in enumerate(dutch_sentences):\n",
    "            e_sen_words = e_sen.split(\" \")\n",
    "            f_sen_words = english_sentences[idx_e].split(\" \")\n",
    "            l_e = len(e_sen_words)\n",
    "            l_f = len(f_sen_words)\n",
    "\n",
    "            # Compute Normalization\n",
    "            s_total = np.full((l_e),0,dtype=float)\n",
    "            for j in range(l_e): \n",
    "                s_total[j] = 0 \n",
    "                e_word = e_sen_words[j]\n",
    "                for i in range(l_f): \n",
    "                    f_word = f_sen_words[i]\n",
    "                    if e_word == '' :\n",
    "                        continue\n",
    "                    if f_word == '' :\n",
    "                        continue\n",
    "                    e_j = dutch_word_dict[e_word]\n",
    "                    f_i = english_word_dict[f_word]\n",
    "                    s_total[j] += t_e_f_mat[e_j][f_i] * a_i_le_lf_mat[i][j][l_f-1][l_e-1] \n",
    "                \n",
    "            # Collect Counts\n",
    "            for j in range(l_e): \n",
    "                e_word = e_sen_words[j]\n",
    "                for i in range(l_f):\n",
    "                    f_word = f_sen_words[i]\n",
    "                    if e_word == '' :\n",
    "                        continue\n",
    "                    if f_word == '' :\n",
    "                        continue\n",
    "                    e_j = dutch_word_dict[e_word]\n",
    "                    f_i = english_word_dict[f_word]\n",
    "\n",
    "                    c = t_e_f_mat[e_j][f_i] * a_i_le_lf_mat[i][j][l_f-1][l_e-1] / s_total[j] \n",
    "                    count_e_f[e_j][f_i] += c\n",
    "                    total_f[f_i] += c \n",
    "                    count_a_i_le_lf[i][j][l_f-1][l_e-1] += c \n",
    "                    total_a_j_le_lf[j][l_e-1][l_f-1] += c \n",
    "\n",
    "        # Estimate Probabilities without Smoothing\n",
    "#         t_e_f_mat = np.full((num_of_e_word, num_of_f_word), 0,dtype=float) \n",
    "#         a_i_le_lf_mat = np.zeros((max_lf, max_le, max_lf,max_le), dtype=float) \n",
    "#         for f_idx in  range(num_of_f_word): \n",
    "#             for e_idx in range(num_of_e_word): \n",
    "#                 if count_e_f[e_idx][f_idx] != 0 :\n",
    "#                     t_e_f_mat[e_idx][f_idx] = (count_e_f[e_idx][f_idx]) / (total_f[f_idx])\n",
    "    \n",
    "#         print(str(datetime.now()))\n",
    "#         for i in range(max_lf):\n",
    "#             for  j in range(max_le):\n",
    "#                 for le in range(max_le):\n",
    "#                     for lf in range(max_lf):\n",
    "#                         if count_a_i_le_lf[i][j][lf][le] != 0 :\n",
    "#                             a_i_le_lf_mat[i][j][lf][le] = (count_a_i_le_lf[i][j][lf][le]) / (total_a_j_le_lf[j][le][lf])\n",
    "            \n",
    "        # Estimate Probabilities with Smoothing\n",
    "        t_e_f_mat = np.full((num_of_e_word, num_of_f_word), 0,dtype=float) \n",
    "        a_i_le_lf_mat = np.zeros((max_lf, max_le, max_lf,max_le), dtype=float) \n",
    "        for f_idx in  range(num_of_f_word): \n",
    "            for e_idx in range(num_of_e_word): \n",
    "                if count_e_f[e_idx][f_idx] != 0 :\n",
    "                    t_e_f_mat[e_idx][f_idx] = (count_e_f[e_idx][f_idx]+1) / (total_f[f_idx]+num_of_e_word)\n",
    "    \n",
    "        print(str(datetime.now()))\n",
    "        for i in range(max_lf):\n",
    "            for  j in range(max_le):\n",
    "                for le in range(max_le):\n",
    "                    for lf in range(max_lf):\n",
    "                        if count_a_i_le_lf[i][j][lf][le] != 0 :\n",
    "                            a_i_le_lf_mat[i][j][lf][le] = (count_a_i_le_lf[i][j][lf][le]+1) / (total_a_j_le_lf[j][le][lf]+num_of_e_word)\n",
    "\n",
    "    print(t_e_f_mat)\n",
    "    print(\"IBMModel2 Training Complete !\")\n",
    "    return t_e_f_mat, a_i_le_lf_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_v28yGgCOW1"
   },
   "source": [
    "This method tokenizes the sentences into lists after converting the sentence into lowercase and removing the punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.518828Z",
     "start_time": "2019-11-15T14:03:03.441099Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lrmT96M2MwP1"
   },
   "outputs": [],
   "source": [
    "def sentences_tokenized(max_index, sentences):\n",
    "    token_sentences = list()\n",
    "    word_dictionary = {} \n",
    "    rev_dictionary = {}\n",
    "    lang_order = 0\n",
    "    cnt = 0\n",
    "    max_len_sentence = 0\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    for row in sentences[:max_index]:\n",
    "        row = row.translate(translate_table)\n",
    "        tokens = word_tokenize(row.lower())\n",
    "\n",
    "        if len(tokens) > max_len_sentence :\n",
    "            max_len_sentence = len(tokens)\n",
    "\n",
    "        produced_sentence = \"\"\n",
    "        for token in tokens:\n",
    "            if token not in word_dictionary:\n",
    "                word_dictionary[token] = lang_order\n",
    "                rev_dictionary[lang_order] = token\n",
    "                lang_order += 1\n",
    "            produced_sentence = produced_sentence + token + \" \"\n",
    "        produced_sentence = produced_sentence[:(len(produced_sentence) - 1)] \n",
    "\n",
    "        token_sentences.append(produced_sentence)\n",
    "\n",
    "\n",
    "    return token_sentences, word_dictionary, rev_dictionary, max_len_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuU8NbE_Ot0H"
   },
   "source": [
    "This method trains our model by taking a portion of the given dataset( say 10000 sentences ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.606579Z",
     "start_time": "2019-11-15T14:03:03.521541Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aEglQJSGMwP5"
   },
   "outputs": [],
   "source": [
    "def train_models(train_model2):\n",
    "    with open(\"Dutch_Updated.txt\", encoding=\"utf8\") as f:\n",
    "            sentences_dut = f.readlines()\n",
    "    with open(\"English_Updated.txt\", encoding=\"utf8\") as f:\n",
    "            sentences_eng = f.readlines()\n",
    "\n",
    "    sentences_dut_red = list()\n",
    "    sentences_eng_red = list()\n",
    "    for sen_idx in range(len(sentences_eng)):\n",
    "        if sen_idx > 500000 :\n",
    "            break\n",
    "        cur_eng_sen = sentences_eng[sen_idx].split()\n",
    "        if len(cur_eng_sen) < 11:\n",
    "            sentences_dut_red.append(sentences_dut[sen_idx])\n",
    "            sentences_eng_red.append(sentences_eng[sen_idx])\n",
    "    \n",
    "    sentences_eng = sentences_eng_red.copy()\n",
    "    sentences_dut = sentences_dut_red.copy()\n",
    "\n",
    "    num_of_train_samples = 10000\n",
    "\n",
    "    # Tokenizing the Dutch Training Samples\n",
    "    dutch_sentences, dutch_word_dict, opp_dutch_word_dict, max_le = sentences_tokenized(num_of_train_samples, sentences_dut)\n",
    "\n",
    "    # Tokenizing the English Training Samples\n",
    "    english_sentences, english_word_dict, opp_english_word_dict, max_lf = sentences_tokenized(num_of_train_samples, sentences_eng)\n",
    "\n",
    "    np.save(\"models/dut_word_dict\",dutch_word_dict)\n",
    "    np.save(\"models/eng_word_dict\",english_word_dict)\n",
    "    \n",
    "    t_e_f  = ibm_model_1(dutch_sentences,english_sentences,dutch_word_dict,english_word_dict)\n",
    "\n",
    "    print(t_e_f.shape)\n",
    "    \n",
    "    num_of_dut_word = t_e_f.shape[0]\n",
    "    num_of_eng_word = t_e_f.shape[1]\n",
    "    Dict_eng = {}\n",
    "    Dict_dut = {}\n",
    "    \n",
    "    for eng_idx in range(num_of_eng_word): \n",
    "        maximum = -1\n",
    "        i = 0\n",
    "        for dut_idx in range(num_of_dut_word): \n",
    "            if t_e_f[dut_idx][eng_idx] > maximum : \n",
    "                maximum = t_e_f[dut_idx][eng_idx]\n",
    "                i = dut_idx\n",
    "\n",
    "        Dict_eng[opp_english_word_dict[eng_idx]] = opp_dutch_word_dict[i]\n",
    "        \n",
    "    for dut_idx in range(num_of_dut_word): \n",
    "        maximum = -1\n",
    "        i = 0\n",
    "        for eng_idx in range(num_of_eng_word):\n",
    "            if t_e_f[dut_idx][eng_idx] > maximum : \n",
    "                maximum = t_e_f[dut_idx][eng_idx]\n",
    "                i = eng_idx\n",
    "\n",
    "        Dict_dut[opp_dutch_word_dict[dut_idx]] = opp_english_word_dict[i]\n",
    "        \n",
    "    np.save(\"models/t_e_f_model1\",t_e_f)\n",
    "    np.save(\"models/dut_max_word_dict_1\",Dict_dut)\n",
    "    np.save(\"models/eng_max_word_dict_1\",Dict_eng)\n",
    "    \n",
    "    if train_model2 == True :\n",
    "        t_e_f, a_i_le_lf_mat = ibm_model_2(t_e_f,dutch_sentences,english_sentences,dutch_word_dict,english_word_dict,max_le,max_lf)\n",
    "        np.save(\"models/t_e_f_model2\",t_e_f)\n",
    "        np.save(\"models/a_i_le_lf_model2\",a_i_le_lf_mat)\n",
    "        num_of_dut_word = t_e_f.shape[0]\n",
    "        num_of_eng_word = t_e_f.shape[1]\n",
    "        Dict_eng = {}\n",
    "        Dict_dut = {}\n",
    "\n",
    "        for eng_idx in range(num_of_eng_word): \n",
    "            maximum = -1\n",
    "            i = 0\n",
    "            for dut_idx in range(num_of_dut_word): \n",
    "                if t_e_f[dut_idx][eng_idx] > maximum : \n",
    "                    maximum = t_e_f[dut_idx][eng_idx]\n",
    "                    i = dut_idx\n",
    "\n",
    "            Dict_eng[opp_english_word_dict[eng_idx]] = opp_dutch_word_dict[i]\n",
    "\n",
    "        for dut_idx in range(num_of_dut_word): \n",
    "            maximum = -1\n",
    "            i = 0\n",
    "            for eng_idx in range(num_of_eng_word):\n",
    "                if t_e_f[dut_idx][eng_idx] > maximum : \n",
    "                    maximum = t_e_f[dut_idx][eng_idx]\n",
    "                    i = eng_idx\n",
    "\n",
    "            Dict_dut[opp_dutch_word_dict[dut_idx]] = opp_english_word_dict[i]\n",
    "        np.save(\"models/dut_max_word_dict_2\",Dict_dut)\n",
    "        np.save(\"models/eng_max_word_dict_2\",Dict_eng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aNlrwizTqb1"
   },
   "source": [
    "This segment of code returns the translated sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.690991Z",
     "start_time": "2019-11-15T14:03:03.612052Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JD1s5-1AMwP9"
   },
   "outputs": [],
   "source": [
    "def get_tokens_of_sentence(sentence):\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    sentence = sentence.translate(translate_table)\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def testing_sentence(sentence_to_translate,max_word_dict):\n",
    "\n",
    "    f_sentence = get_tokens_of_sentence(sentence_to_translate)\n",
    "    e_sentence = \"\"\n",
    "    for word in f_sentence :\n",
    "        if word in max_word_dict:\n",
    "            e_sentence = e_sentence + max_word_dict[word] + \" \"\n",
    "        else:\n",
    "            print(\"word '\"+ word +\"' is not found in target language dictionary\")\n",
    "            continue\n",
    "\n",
    "    return e_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufmzCWWsPCBp"
   },
   "source": [
    "The method below calculates the cosine similarity and jaccard coefficient between any two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:03.773848Z",
     "start_time": "2019-11-15T14:03:03.697945Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7oEokAF_MwQB"
   },
   "outputs": [],
   "source": [
    "def vector_similarity(string1, string2,len_dict):  \n",
    "\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    string2 = string2.translate(translate_table)\n",
    "    \n",
    "    X_list = word_tokenize(string1)  \n",
    "    Y_list = word_tokenize(string2.lower())\n",
    "       \n",
    "    X =[];Y =[]\n",
    "     \n",
    "    X_set = {w for w in X_list }  \n",
    "    Y_set = {w for w in Y_list }\n",
    "     \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector:\n",
    "        if w in X_set: \n",
    "            X.append(1) \n",
    "        else: \n",
    "            X.append(0)\n",
    "        if w in Y_set:\n",
    "            Y.append(1)\n",
    "        else: \n",
    "            Y.append(0)\n",
    "    \n",
    "    len_dict = len(rvector)\n",
    "    n = len(X)\n",
    "    sum_X = 0\n",
    "    sum_Y = 0\n",
    "    sum_XY = 0\n",
    "    squareSum_X = 0\n",
    "    squareSum_Y = 0\n",
    "     \n",
    "    i = 0\n",
    "    while i < n :\n",
    "        sum_X = sum_X + X[i]         \n",
    "        sum_Y = sum_Y + Y[i]\n",
    "        sum_XY = sum_XY + X[i] * Y[i]\n",
    "         \n",
    "        squareSum_X = squareSum_X + X[i] * X[i]\n",
    "        squareSum_Y = squareSum_Y + Y[i] * Y[i]\n",
    "         \n",
    "        i = i + 1\n",
    "    \n",
    "    c = 0\n",
    "#     if (len_dict * squareSum_X - sum_X * sum_X) == 0 or (len_dict * squareSum_Y - sum_Y * sum_Y) == 0 :\n",
    "#         corr = 1 \n",
    "#     else :    \n",
    "#         corr = (float)(len_dict * sum_XY - sum_X * sum_Y)/(float)(math.sqrt((len_dict * squareSum_X - sum_X * sum_X) * (len_dict * squareSum_Y - sum_Y * sum_Y)))        \n",
    "    \n",
    "    jac = float(len(X_set & Y_set)) / len(X_set | Y_set)\n",
    "    \n",
    "    for i in range(len(rvector)):\n",
    "        c += X[i]*Y[i]\n",
    "    if sum(X) == 0 or sum(Y) == 0 :\n",
    "        cosine = 0\n",
    "        print(\"test\")\n",
    "    else :    \n",
    "        cosine = c / float((sum(X)*sum(Y))**0.5)\n",
    "    \n",
    "    print(\"Cosine Similarity: \", cosine)\n",
    "    print(\"Jaccard Coefficient: \", jac)\n",
    "    \n",
    "    return cosine , jac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2FtsURPPk_h"
   },
   "source": [
    "The method below is used for testing our model on training and validation dataset. It takes into account the direction of translation as specified by the user(dutch to english or english to dutch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:18:13.089790Z",
     "start_time": "2019-11-15T14:18:13.050493Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "g3gbu53OMwQG"
   },
   "outputs": [],
   "source": [
    "def test_model(lang1_to_lang2, type_set, model_no):\n",
    "\n",
    "    e_word_dict = np.load(\"models/dut_word_dict.npy\",allow_pickle = True).item()\n",
    "    f_word_dict = np.load(\"models/eng_word_dict.npy\",allow_pickle = True).item()\n",
    "    \n",
    "    len_eng_dict = len(e_word_dict)\n",
    "    len_dut_dict = len(f_word_dict)\n",
    "\n",
    "    if model_no == 1 :\n",
    "        e_max_word_dict = np.load(\"models/dut_max_word_dict_1.npy\",allow_pickle = True).item()\n",
    "        f_max_word_dict = np.load(\"models/eng_max_word_dict_1.npy\",allow_pickle = True).item()\n",
    "    elif model_no == 2 :\n",
    "        e_max_word_dict = np.load(\"models/dut_max_word_dict_2.npy\",allow_pickle = True).item()\n",
    "        f_max_word_dict = np.load(\"models/eng_max_word_dict_2.npy\",allow_pickle = True).item()\n",
    "        \n",
    "    no_of_train_samples = 10000\n",
    "\n",
    "    if lang1_to_lang2 == \"eng_to_dut\" and type_set == 2 :\n",
    "        with open(\"d1.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(\"e1.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "        \n",
    "        sentences_dut_red = list()\n",
    "        sentences_eng_red = list()\n",
    "        for sen_idx in range(len(sentences_eng)):\n",
    "            if sen_idx > 500000 :\n",
    "                break\n",
    "            cur_eng_sen = sentences_eng[sen_idx].split()\n",
    "            if len(cur_eng_sen) < 11:\n",
    "                sentences_dut_red.append(sentences_dut[sen_idx])\n",
    "                sentences_eng_red.append(sentences_eng[sen_idx])\n",
    "\n",
    "        cos_sim = 0 \n",
    "        jac_sim = 0\n",
    "        i = no_of_train_samples - 1\n",
    "        no_of_samples = 100\n",
    "        for sentence in sentences_eng[no_of_train_samples:(no_of_train_samples + no_of_samples)]:\n",
    "            i = i+1\n",
    "            sen_temp = testing_sentence(sentence,f_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_similarity(sen_temp,sentences_dut[i],len_dut_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_dut[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "        \n",
    "        cos_sim = cos_sim/no_of_samples\n",
    "        jac_sim = jac_sim/no_of_samples\n",
    "        print(\"Average Cosine Similiarity for Validation Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Validation Set\" ,jac_sim)\n",
    "        \n",
    "    elif lang1_to_lang2 == \"dut_to_eng\" and type_set == 2 :\n",
    "        with open(\"Dutch_Updated.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(\"English_Updated.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "        \n",
    "        sentences_dut_red = list()\n",
    "        sentences_eng_red = list()\n",
    "        for sen_idx in range(len(sentences_dut)):\n",
    "            if sen_idx > 500000 :\n",
    "                break\n",
    "            cur_dut_sen = sentences_dut[sen_idx].split()\n",
    "            if len(cur_dut_sen) < 11:\n",
    "                sentences_dut_red.append(sentences_dut[sen_idx])\n",
    "                sentences_eng_red.append(sentences_eng[sen_idx])\n",
    "\n",
    "        cos_sim = 0 \n",
    "        jac_sim = 0\n",
    "        i = no_of_train_samples - 1\n",
    "        no_of_samples = 100\n",
    "        for sentence in sentences_dut[no_of_train_samples:(no_of_train_samples + no_of_samples)]:\n",
    "            i = i+1\n",
    "            sen_temp = testing_sentence(sentence,e_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_similarity(sen_temp,sentences_eng[i],len_eng_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_eng[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "        \n",
    "        cos_sim = cos_sim/no_of_samples\n",
    "        jac_sim = jac_sim/no_of_samples\n",
    "        print(\"Average Cosine Similiarity for Validation Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Validation Set\" ,jac_sim)\n",
    "        \n",
    "    elif lang1_to_lang2 == \"eng_to_dut\" and type_set == 1 :\n",
    "        with open(\"d4.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(\"e4.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "        \n",
    "        sentences_dut_red = list()\n",
    "        sentences_eng_red = list()\n",
    "        for sen_idx in range(len(sentences_eng)):\n",
    "            if sen_idx > 500000 :\n",
    "                break\n",
    "            cur_eng_sen = sentences_eng[sen_idx].split()\n",
    "            sentences_dut_red.append(sentences_dut[sen_idx])\n",
    "            sentences_eng_red.append(sentences_eng[sen_idx])\n",
    "\n",
    "        cos_sim = 0 \n",
    "        jac_sim = 0\n",
    "        i = -1\n",
    "        no_of_samples = 100\n",
    "        for sentence in sentences_eng[ 0 : 10 ]:\n",
    "            i = i+1\n",
    "            sen_temp = testing_sentence(sentence,f_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_similarity(sen_temp,sentences_dut[i],len_dut_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_dut[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "        \n",
    "        cos_sim = cos_sim/10\n",
    "        jac_sim = jac_sim/10\n",
    "        print(\"Average Cosine Similiarity for Training Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Training Set\" ,jac_sim)\n",
    "    \n",
    "    elif lang1_to_lang2 == \"dut_to_eng\" and type_set == 1 :\n",
    "        with open(\"d1.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(\"e1.txt\", encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "        \n",
    "        sentences_dut_red = list()\n",
    "        sentences_eng_red = list()\n",
    "        for sen_idx in range(len(sentences_dut)):\n",
    "            if sen_idx > 500000 :\n",
    "                break\n",
    "            cur_dut_sen = sentences_dut[sen_idx].split()\n",
    "            sentences_dut_red.append(sentences_dut[sen_idx])\n",
    "            sentences_eng_red.append(sentences_eng[sen_idx])\n",
    "\n",
    "        cos_sim = 0 \n",
    "        jac_sim = 0\n",
    "        i = -1\n",
    "        no_of_samples = 100\n",
    "        for sentence in sentences_dut[ 0 : no_of_train_samples ]:\n",
    "            i = i+1\n",
    "            sen_temp = testing_sentence(sentence,e_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_similarity(sen_temp,sentences_eng[i],len_eng_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_eng[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "        \n",
    "        cos_sim = cos_sim/10\n",
    "        jac_sim = jac_sim/10\n",
    "        print(\"Average Cosine Similiarity for Training Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Training Set\" ,jac_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxODPLfDVKrN"
   },
   "source": [
    "The method below is used for testing our model on unknown dataset. It takes into account the direction of translation as specified by the user(dutch to english or english to dutch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:18:14.520126Z",
     "start_time": "2019-11-15T14:18:14.512572Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HTtKvSVqUXMj"
   },
   "outputs": [],
   "source": [
    "def test_model_new(lang1_to_lang2,dutch_file,english_file,model_no):\n",
    "\n",
    "    e_word_dict = np.load(\"models/dut_word_dict.npy\",allow_pickle = True).item()\n",
    "    f_word_dict = np.load(\"models/eng_word_dict.npy\",allow_pickle = True).item()\n",
    "    \n",
    "    len_eng_dict = len(e_word_dict)\n",
    "    len_dut_dict = len(f_word_dict)\n",
    "    \n",
    "    if model_no == 1 :\n",
    "        e_max_word_dict = np.load(\"models/dut_max_word_dict_1.npy\",allow_pickle = True).item()\n",
    "        f_max_word_dict = np.load(\"models/eng_max_word_dict_1.npy\",allow_pickle = True).item()\n",
    "    elif model_no == 2 :\n",
    "        e_max_word_dict = np.load(\"models/dut_max_word_dict_2.npy\",allow_pickle = True).item()\n",
    "        f_max_word_dict = np.load(\"models/eng_max_word_dict_2.npy\",allow_pickle = True).item()\n",
    "    \n",
    "    if lang1_to_lang2 == \"eng_to_dut\" :\n",
    "        with open(dutch_file, encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(english_file, encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "        \n",
    "        cos_sim = 0\n",
    "        jac_sim = 0\n",
    "        i = 0\n",
    "        for sentence in sentences_eng :\n",
    "            i = i+1\n",
    "            sen_temp = sentence_tester(sentence,f_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_acc(sen_temp,sentences_dut[i],len_dut_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_dut[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "        \n",
    "        cos_sim = cos_sim/i\n",
    "        jac_sim = jac_sim/i\n",
    "        print(\"Average Cosine Similiarity for Test Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Test Set\" ,jac_sim)\n",
    "    elif lang1_to_lang2 == \"eng_to_dut\" :\n",
    "        with open(dutch_file, encoding=\"utf8\") as f:\n",
    "                sentences_dut = f.readlines()\n",
    "        with open(english_file, encoding=\"utf8\") as f:\n",
    "                sentences_eng = f.readlines()\n",
    "\n",
    "        cos_sim = 0\n",
    "        jac_sim = 0\n",
    "        i = 0\n",
    "        for sentence in sentences_dut :\n",
    "            i = i+1\n",
    "            sen_temp = sentence_tester(sentence,e_max_word_dict)\n",
    "            cos_temp,jac_temp = vector_acc(sen_temp,sentences_eng[i],len_eng_dict)\n",
    "            if cos_temp == 0 :\n",
    "                print(sentences_eng[i])\n",
    "                print(sen_temp)\n",
    "            cos_sim = cos_sim + cos_temp\n",
    "            jac_sim = jac_sim + jac_temp\n",
    "\n",
    "        cos_sim = cos_sim/i\n",
    "        jac_sim = jac_sim/i\n",
    "        print(\"Average Cosine Similiarity for Test Set\" ,cos_sim)\n",
    "        print(\"Average Jaccard Coefficient for Test Set\" ,jac_sim)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIgVCZ5hPRAB"
   },
   "source": [
    "This is the program interface as visible to the user. Once the user runs the code, he/she is prompted to enter a choice between 1 to 4. Choice 1 allows the user to train the data, choice 2 allows for testing From English to Dutch, choice 3 allows for testing From Dutch to English. Additionally, by choosing option 4, user can exit the program.\n",
    "Upon the selection of either option 2 or 3, the user is allowed to choose a model of his/her choice - IBM model 1 or IBM model 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-15T14:18:15.671Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33955,
     "status": "ok",
     "timestamp": 1573230340369,
     "user": {
      "displayName": "VITTHAL BHANDARI",
      "photoUrl": "",
      "userId": "13632601739833432191"
     },
     "user_tz": -330
    },
    "id": "xVAYW4_AMwQK",
    "outputId": "443f1bba-0c63-4a44-ddff-46d30170eacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice: \n",
      "\t1: Training \n",
      "\t2: Testing From English to Dutch \n",
      "\t3: Testing From Dutch to English \n",
      "\t4: Testing for Test Set \n",
      "\t5:Exit\n",
      "3\n",
      "\n",
      "Enter the type of dataset to be tested: \n",
      "\t1: Training \n",
      "\t2: Validation \n",
      "1\n",
      "\n",
      "Enter the Model to be tested: \n",
      "\t1:IBM Model 1 \n",
      "\t2:IBM Model 2 \n",
      "1\n",
      "Cosine Similarity:  0.3535533905932738\n",
      "Jaccard Coefficient:  0.21428571428571427\n",
      "Cosine Similarity:  0.5430450802499489\n",
      "Jaccard Coefficient:  0.3695652173913043\n",
      "word 'rechtsreeks' is not found in target language dictionary\n",
      "Cosine Similarity:  0.5151221963699317\n",
      "Jaccard Coefficient:  0.34375\n",
      "word 'zwaarste' is not found in target language dictionary\n",
      "word 'vogelstand' is not found in target language dictionary\n",
      "Cosine Similarity:  0.6482037235521644\n",
      "Jaccard Coefficient:  0.47619047619047616\n",
      "word 'vogelbescherming' is not found in target language dictionary\n",
      "word 'schat' is not found in target language dictionary\n",
      "word 'vogels' is not found in target language dictionary\n",
      "word 'zeekoeten' is not found in target language dictionary\n",
      "Cosine Similarity:  0.14907119849998599\n",
      "Jaccard Coefficient:  0.07692307692307693\n",
      "word 'onthutsende' is not found in target language dictionary\n",
      "word 'vogels' is not found in target language dictionary\n",
      "word 'dierenarts' is not found in target language dictionary\n",
      "Cosine Similarity:  0.25717224993681986\n",
      "Jaccard Coefficient:  0.14705882352941177\n",
      "word 'strengst' is not found in target language dictionary\n",
      "Cosine Similarity:  0.19446111706564934\n",
      "Jaccard Coefficient:  0.10714285714285714\n",
      "Cosine Similarity:  0.31819805153394637\n",
      "Jaccard Coefficient:  0.1875\n",
      "Cosine Similarity:  0.5666666666666667\n",
      "Jaccard Coefficient:  0.3953488372093023\n",
      "word 'politica' is not found in target language dictionary\n",
      "Cosine Similarity:  0.35714285714285715\n",
      "Jaccard Coefficient:  0.21428571428571427\n",
      "Average Cosine Similiarity for Training Set 0.3902636531611244\n",
      "Average Jaccard Coefficient for Training Set 0.25320507169578577\n",
      "\n",
      "Enter your choice: \n",
      "\t1: Training \n",
      "\t2: Testing From English to Dutch \n",
      "\t3: Testing From Dutch to English \n",
      "\t4: Testing for Test Set \n",
      "\t5:Exit\n",
      "2\n",
      "\n",
      "Enter the type of dataset to be tested: \n",
      "\t1: Training \n",
      "\t2: Validation \n",
      "1\n",
      "\n",
      "Enter the Model to be tested: \n",
      "\t1:IBM Model 1 \n",
      "\t2:IBM Model 2 \n",
      "1\n",
      "Cosine Similarity:  0.2508726030021272\n",
      "Jaccard Coefficient:  0.14285714285714285\n",
      "Cosine Similarity:  0.5652173913043478\n",
      "Jaccard Coefficient:  0.3939393939393939\n",
      "Cosine Similarity:  0.4879500364742666\n",
      "Jaccard Coefficient:  0.3225806451612903\n",
      "Cosine Similarity:  0.4708709557974187\n",
      "Jaccard Coefficient:  0.30434782608695654\n",
      "word 'ie' is not found in target language dictionary\n",
      "word 'inputs' is not found in target language dictionary\n",
      "word '2020' is not found in target language dictionary\n",
      "Cosine Similarity:  0.31622776601683794\n",
      "Jaccard Coefficient:  0.16666666666666666\n",
      "word 'knell' is not found in target language dictionary\n",
      "Cosine Similarity:  0.3481553119113957\n",
      "Jaccard Coefficient:  0.21052631578947367\n",
      "word 'imperfect' is not found in target language dictionary\n",
      "Cosine Similarity:  0.44901325506693723\n",
      "Jaccard Coefficient:  0.28846153846153844\n",
      "word 'rhine' is not found in target language dictionary\n",
      "word 'elbe' is not found in target language dictionary\n",
      "word 'polluted' is not found in target language dictionary\n",
      "Cosine Similarity:  0.5111012519999519\n",
      "Jaccard Coefficient:  0.3404255319148936\n",
      "Cosine Similarity:  0.4377137796365057\n",
      "Jaccard Coefficient:  0.28\n",
      "Cosine Similarity:  0.0\n",
      "Jaccard Coefficient:  0.0\n",
      "Dit brengt zonder enige twijfel grote kosten met zich mee.\n",
      "boeren hebben geuite een aantal van betreft meer de verleden enkele dagen \n",
      "Average Cosine Similiarity for Training Set 0.38371223512097885\n",
      "Average Jaccard Coefficient for Training Set 0.2449805060877356\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        mode = int(input('\\nEnter your choice: \\n\\t1: Training \\n\\t2: Testing From English to Dutch \\n\\t3: Testing From Dutch to English \\n\\t4: Testing for Test Set \\n\\t5:Exit\\n'))\n",
    "    except ValueError:\n",
    "        print (\"Not a number\")\n",
    "\n",
    "    if mode == 1:\n",
    "        train_model2 = int(input(\"\\nEnter choice of training: \\n\\t1:Both Model 1 and 2 \\n\\t2:Only Model1 \\n\"))\n",
    "        if train_model2 == 1 :\n",
    "            train_models(True)\n",
    "        elif train_model2 == 2 :\n",
    "            train_models(False)\n",
    "    elif mode == 2:\n",
    "        type_set = int(input(\"\\nEnter the type of dataset to be tested: \\n\\t1: Training \\n\\t2: Validation \\n\"))\n",
    "        lang1_to_lang2 = \"eng_to_dut\"\n",
    "        model_no = int(input(\"\\nEnter the Model to be tested: \\n\\t1:IBM Model 1 \\n\\t2:IBM Model 2 \\n\"))\n",
    "        test_model(lang1_to_lang2,type_set,model_no)\n",
    "    elif mode == 3:\n",
    "        type_set = int(input(\"\\nEnter the type of dataset to be tested: \\n\\t1: Training \\n\\t2: Validation \\n\"))\n",
    "        lang1_to_lang2 = \"dut_to_eng\"\n",
    "        model_no = int(input(\"\\nEnter the Model to be tested: \\n\\t1:IBM Model 1 \\n\\t2:IBM Model 2 \\n\"))        \n",
    "        test_model(lang1_to_lang2,type_set,model_no)\n",
    "    elif mode == 4:\n",
    "        dutch_file = input(\"\\nEnter the name of Dutch file\")\n",
    "        english_file = input(\"\\nEnter the name of English file\")\n",
    "        lang = input(\"\\nEnter the language to be translated from \\n\\tEnglish - eng \\n\\tDutch - dut\")\n",
    "        if lang == \"eng\" :\n",
    "            lang1_to_lang2 = \"eng_to_dut\"\n",
    "        elif lang == \"dut\" :\n",
    "            lang1_to_lang2 = \"dut_to_eng\"\n",
    "        model_no = int(input(\"\\nEnter the Model to be tested: \\n\\t1:IBM Model 1 \\n\\t2:IBM Model 2 \\n\"))\n",
    "        test_model_new(lang1_to_lang2,dutch_file,english_file,model_no)  \n",
    "    elif mode == 5:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid Choice\")\n",
    "\n",
    "print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR_Assignment.ipynb",
   "provenance": [
    {
     "file_id": "17Y9OQ3txAvNfXwrvBOdDMwgQeM6H0zh0",
     "timestamp": 1573047873687
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
